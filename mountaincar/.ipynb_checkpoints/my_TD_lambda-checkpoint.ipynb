{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#This implementation is based on Sarsa(lambda) algorithm from (http://www-anw.cs.umass.edu/~barto/courses/cs687/)\n",
    "\n",
    "\n",
    "# https://deeplearningcourses.com/c/deep-reinforcement-learning-in-python\n",
    "# https://www.udemy.com/deep-reinforcement-learning-in-python\n",
    "from __future__ import print_function, division\n",
    "from builtins import range\n",
    "# Note: you may need to update your version of future\n",
    "# sudo pip install -U future\n",
    "#\n",
    "# Note: gym changed from version 0.7.3 to 0.8.0\n",
    "# MountainCar episode length is capped at 200 in later versions.\n",
    "# This means your agent can't learn as much in the earlier episodes\n",
    "# since they are no longer as long.\n",
    "#\n",
    "# Adapt Q-Learning script to use TD(lambda) method instead\n",
    "\n",
    "import gym\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from gym import wrappers\n",
    "from datetime import datetime\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "\n",
    "# code we already wrote\n",
    "from q_learning import plot_cost_to_go, FeatureTransformer, plot_running_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SubModel:\n",
    "    def __init__(self, env, featureTransformer, lumda, learningRate, discount):\n",
    "        self.env = env        \n",
    "        self.ft = featureTransformer(env)\n",
    "        self.lumda = lumda\n",
    "        self.learningRate = learningRate\n",
    "        self.discount = discount\n",
    "        \n",
    "        #set size of the input vector\n",
    "        X = self.ft.transform([env.observation_space.sample()])\n",
    "        self.vSize = X.size       \n",
    "        self.el = np.zeros(self.vSize)\n",
    "        self.w = np.zeros(self.vSize)\n",
    "        \n",
    "        #create a sub-model for each action\n",
    "        self.actionSize  = env.action_space.n\n",
    "        \n",
    "    def predict(self, status):\n",
    "        X = self.ft.transfor([status])\n",
    "        Y = self.w.dot(X)\n",
    "        return Y\n",
    "    \n",
    "    def partial_fit(self, reward, oldStatus, newValue):\n",
    "        oldValue = self.predict(oldStatus)\n",
    "        self.el = self.lumda * self.discount * self.el - self.ft.transform([oldStatus])\n",
    "        diff = reward + self.discount * newValue - oldValue\n",
    "        self.w += self.learningRate * diff * self.el\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Models:\n",
    "    def __init__(self, env, featureTransformer, lumda, learningRate, discount):\n",
    "        self.env = env        \n",
    "        self.ft = featureTransformer(env)      \n",
    "           \n",
    "        \n",
    "        #create a sub-model for each action\n",
    "        self.actionSize  = env.action_space.n\n",
    "        self.models = []\n",
    "        for i in range(self.actionSize):\n",
    "            self.models.append(SubModel(env, featureTransformer, lumda, learningRate, discount))\n",
    "\n",
    "    def update(self, reward, oldStatus, newStatus, action, newAction):        \n",
    "        newValue = self.models[newAction].predict(newStatus)\n",
    "        self.models[action].partial_fit(reward, oldStatus, newValue)\n",
    "        \n",
    "    def predict(self, status): \n",
    "        results = np.stack([m.predict(status) for m in models])\n",
    "        return results\n",
    "    \n",
    "    def sample_action(self, status, eps):\n",
    "        if np.random.random() < eps:\n",
    "            return self.env.action_space.sample()\n",
    "        else:\n",
    "            return np.argmax(self.predict(status))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def playOne(env, models, eps):\n",
    "    oldObs = env.reset()\n",
    "    models.initEligibility()\n",
    "    done = False    \n",
    "    while not done:        \n",
    "        action = models.sample_action(oldObs, eps)\n",
    "        newObs, reward, done, info = env.step(action)\n",
    "        nextValue = np.max(models.predict(newObs))\n",
    "        models.update(reward, oldObs, action, nextValue)\n",
    "        oldObs = newObs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    env = gym.make('MountainCar-v0')\n",
    "    env.reset()\n",
    "    ft = FeatureTransformer(env)    \n",
    "    model = Model(env, ft)\n",
    "    for i in range(100):\n",
    "        playOne(env, model)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
